{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_style('ticks')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model, datasets\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get and see data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'DESCR', 'feature_names'])\n",
      "Column names and what each column in data represents: ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
      "data shape:  (442, 10)\n",
      "Target:  (442,)\n"
     ]
    }
   ],
   "source": [
    "diabetes = datasets.load_diabetes()  # load dataset\n",
    "print(diabetes.keys())\n",
    "print('Column names and what each column in data represents:', diabetes['feature_names'])\n",
    "print('data shape: ', diabetes.data.shape)\n",
    "print('Target: ', diabetes.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment for dataset description\n",
    "#print(diabetes['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:  [938.23786125]\n",
      "Mean squared error:2548.07\n",
      "Variance score: 0.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivaspepe/.pyenv/versions/3.6.3/lib/python3.6/site-packages/sklearn/linear_model/base.py:509: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  linalg.lstsq(X, y)\n"
     ]
    }
   ],
   "source": [
    "diabetes_X = diabetes.data[:, np.newaxis, 2]  # reshape to column vector\n",
    "\n",
    "# train data\n",
    "train_X = diabetes_X[:-20]\n",
    "test_X = diabetes_X[-20:]\n",
    "\n",
    "# test data\n",
    "train_y = diabetes.target[:-20]\n",
    "test_y = diabetes.target[-20:]\n",
    "\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: ', model.coef_)\n",
    "\n",
    "# prediction\n",
    "predict_y = model.predict(test_X)\n",
    "\n",
    "# The mean squared error\n",
    "print('Mean squared error:{:.2f}'.format(mean_squared_error(test_y, predict_y)))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: {:.2f}'.format(r2_score(test_y, predict_y)))\n",
    "\n",
    "# Plot outputs\n",
    "#plt.scatter(train_X, train_y,  color='black',)\n",
    "#plt.plot(test_x, predict_y)\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The equation of line follows the form $\\beta_1x + \\beta_0 = y$\n",
    "<br><center>\n",
    "     $mx + b = y\\\\\n",
    "     \\beta_1x + \\beta_0 = y$\n",
    "</center></br>\n",
    "where our parameters are the y-intercept, b, and slope m.\n",
    "<br>The goal of linear regression is to fit a linear model (i.e. a line) to a pair of data points. A linear model has the form\n",
    "<br><center>\n",
    "     $X_{ij}\\beta_{j,1} = y_{i,1}$\n",
    "</center></br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear model is the simplest model used to explain data (dependent varible $y$). The goal is to find a line, $\\hat{y} = \\beta_1x + \\beta_0 $, which can represent the data trend. Ideally, one wants to find the best possible line (i.e. $y = \\hat{y}$). Then it's apparent that the best possible line would result in <br>\n",
    "<br><center>$y - \\hat{y} = 0$\n",
    "</center></br>\n",
    "</br>\n",
    "\n",
    "For most real life problems we would never get zero, so we try to find a $\\hat{y}$, or parameters $\\beta_j$, which can take us as close to zero as possible, or in other words minimizes the difference which is also called the residuals. Let's see some ways $\\hat{y}$ can be expressed,<br>\n",
    "<br><center>\n",
    "    \\begin{align}\n",
    "     mx + b &= \\hat{y}\\\\\n",
    "     \\beta_1x + \\beta_0 &= \\hat{y}\\\\\n",
    "     X_{ij}\\beta_{j,1} &= \\hat{y}_{i,1}\n",
    "     \\end{align}\n",
    "</center></br>\n",
    "\n",
    "The first two equations are identical and work to describe a relationship between two variables: the independent x and dependent y. The last equation works as a generalization for when we have i equations and j independent variables to explain/model y. An additional convenient the expression of the dimensionality for each term (important to match proper sides for matrix multiplication)\n",
    "- $X_{ij}$ is a matrix of shape $i$ x $j$.\n",
    "- $\\beta_{j,1}$ is a column vector of with $j$ rows and 1 column and it represents all parameters\n",
    "- $\\hat{y}_{i,1}$ is a columm vector and dependent variable\n",
    "\n",
    "Now let's write the residual as $S(\\beta)$ and take the derivative with respect to the parameters to minimize it\n",
    "\n",
    "<br><center>\n",
    "    \\begin{align}\n",
    "    S(\\beta) &= ||y - \\hat{y}||^2\\\\\n",
    "    \\frac{dS}{d\\beta} &= 2X_{ij}^T(y - X_{ij}\\beta_{j,1}) = 0\n",
    "    \\end{align}\n",
    "</center></br> \n",
    "Now we solve for $\\beta$\n",
    "<br><center>\n",
    "    \\begin{align}\n",
    "    0 &= 2X_{ij}^T (y - X_{ij}\\beta_{j,1}) \\\\\n",
    "    0 &= X_{ij}^Ty - X_{ij}^TX_{ij}\\beta_{j,1}\\\\\n",
    "    \\beta_{j,1} &= (X_{ij}^TX_{ij})^{-1}  X_{ij}^Ty\\\\\n",
    "    \\end{align}\n",
    "</center></br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient OLS: [970.16723129]\n",
      "Means Squared Error: 2546.14\n"
     ]
    }
   ],
   "source": [
    "# get parameters and y_intecept\n",
    "param = np.linalg.pinv(train_X.T @ train_X) @ train_X.T @ train_y\n",
    "y_int = train_y.mean() + param * train_X.T.mean()\n",
    "print('Coefficient OLS:', param)\n",
    "\n",
    "# get y_hat\n",
    "y_hat = param * test_X.ravel() + y_int\n",
    "\n",
    "# calculate Mean sq. error\n",
    "MSE = np.mean(np.square(test_y - y_hat))\n",
    "print(\"Means Squared Error: {:.2f}\".format(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
