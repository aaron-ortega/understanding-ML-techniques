{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_style('ticks')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = datasets.load_diabetes()  # load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_X = diabetes.data[:, np.newaxis, 2]  # reshape to column vector\n",
    "\n",
    "# train data\n",
    "train_X = diabetes_X[:-20]\n",
    "test_X = diabetes_X[-20:]\n",
    "\n",
    "# test data\n",
    "train_y = diabetes.target[:-20][:, np.newaxis]\n",
    "test_y = diabetes.target[-20:][:, np.newaxis]\n",
    "\n",
    "# design model\n",
    "reg = linear_model.Lasso(alpha = 0.1)\n",
    "\n",
    "# estimate parameters\n",
    "reg.fit(train_X, train_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick explanation for LASSO regression analysis\n",
    "Good Refs:<br>\n",
    "http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/<br>\n",
    "https://towardsdatascience.com/regularization-in-machine-learning-connecting-the-dots-c6e030bfaddd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similiarly to a Ridge regression, LASSO (Least Absolute Shrinkage and Selection Operator) regression aims to provide the model with addition information in order to make the model more robust. In the case of LASSO, we impose the following constraint to the estimates (our $\\beta$ parameters).\n",
    "\n",
    "<br><center>\n",
    "    \\begin{align}\n",
    "||\\beta|| = \\sum|\\beta_j| \\leq t\n",
    "    \\end{align}\n",
    "</center></br> the regularization parameter is $\\lambda$ and so putting it together\n",
    "\n",
    "<br><center>\n",
    "    \\begin{align}\n",
    "    S(\\beta) &= ||y - X\\beta||^2 + \\lambda||\\beta||\\\\\n",
    "    minimize(S(\\beta)) &= \\frac{dS}{d\\beta} = 0\\\\\n",
    "    \\end{align}\n",
    "</center></br>\n",
    "\n",
    "Unlike with Ridge, the minimization has no closed form so an optimization scheme must be employed.\n",
    "\n",
    "Additionally unlike Ridge, LASSO can result in sparse outputs (i.e. estimates being equal to zero) which serve as a feature selector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
